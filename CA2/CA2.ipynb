{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle \n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import FreqDist\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(allWords):\n",
    "    # preprocess\n",
    "    #remove stop words\n",
    "    stopword = stopwords.words('english')\n",
    "    all_words_without_stopwords = [word.lower() for word in allWords if word not in stopword]\n",
    "    \n",
    "    # preprocess\n",
    "    #remove punctuation\n",
    "    punctuations = '''!()-[]--'{}ØŒ;:'\"\\,<>./?@#$%^&*_~'''\n",
    "    all_words_without_punctuation1 = [word for word in all_words_without_stopwords if word not in string.punctuation]\n",
    "    all_words_without_punctuation = [word for word in all_words_without_punctuation1 if word not in punctuations]\n",
    "    \n",
    "    # preprocess\n",
    "    #remove numbers\n",
    "    all_words_without_numbers = []\n",
    "    for word in all_words_without_punctuation:\n",
    "        x = re.sub(r\"\\d+\", \"\", word)\n",
    "        if(x != ''):\n",
    "            all_words_without_numbers.append(x)\n",
    "            \n",
    "    # preprocess\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    all_words_lemmatized = [lemmatizer.lemmatize(word) for word in all_words_without_numbers]\n",
    "    \n",
    "    return all_words_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(all_train_words):\n",
    "    words = preprocess(all_train_words)\n",
    "    words_frequency = FreqDist(words)\n",
    "#     len(all_words_frequency)\n",
    "#     print(all_words_frequency.most_common(len(all_words_frequency))[len(all_words_frequency)-10000])\n",
    "#     print(all_words_frequency.most_common(len(all_words_frequency))[len(all_words_frequency)-20000])\n",
    "#     print(all_words_frequency.most_common(len(all_words_frequency))[len(all_words_frequency)-25000])\n",
    "#     print(all_words_frequency.most_common(len(all_words_frequency))[len(all_words_frequency)-27000])\n",
    "#     print(all_words_frequency.most_common(len(all_words_frequency))[len(all_words_frequency)-30000])\n",
    "#     print(all_words_frequency.most_common(len(all_words_frequency))[len(all_words_frequency)-31000])\n",
    "    features = words_frequency.most_common(1500)\n",
    "    feature_set = []\n",
    "    labels = []\n",
    "    for review,category in reviews:\n",
    "        row = []\n",
    "        for feature,freq in features:\n",
    "            if feature in review:\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "\n",
    "        feature_set.append(row)\n",
    "        labels.append(category)\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(feature_set,labels)\n",
    "    return gnb,feature_set,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        reviews.append((preprocess(movie_reviews.words(fileid)), category))\n",
    "shuffle(reviews)\n",
    "all_train_words = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395989974937343\n",
      "[[182  24]\n",
      " [ 40 153]]\n"
     ]
    }
   ],
   "source": [
    "for review,count in reviews[0:1600]:\n",
    "    for word in review:\n",
    "        all_train_words.append(word)\n",
    "gnb,feature_set,labels = classification(all_train_words)\n",
    "y_pred = gnb.predict(feature_set[1601:])\n",
    "print(accuracy_score(labels[1601:], y_pred))\n",
    "print(confusion_matrix(labels[1601:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n",
      "[[181  23]\n",
      " [ 40 155]]\n"
     ]
    }
   ],
   "source": [
    "rev = reviews[:1200]+reviews[1601:]\n",
    "for review,count in rev:\n",
    "    for word in review:\n",
    "        all_train_words.append(word)\n",
    "gnb,feature_set,labels = classification(all_train_words)\n",
    "y_pred = gnb.predict(feature_set[1201:1600])\n",
    "print(accuracy_score(labels[1201:1600], y_pred))\n",
    "print(confusion_matrix(labels[1201:1600], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n",
      "[[164  22]\n",
      " [ 35 178]]\n"
     ]
    }
   ],
   "source": [
    "rev = reviews[:800]+reviews[1201:]\n",
    "for review,count in rev:\n",
    "    for word in review:\n",
    "        all_train_words.append(word)\n",
    "gnb,feature_set,labels = classification(all_train_words)\n",
    "y_pred = gnb.predict(feature_set[801:1200])\n",
    "print(accuracy_score(labels[801:1200], y_pred))\n",
    "print(confusion_matrix(labels[801:1200], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8646616541353384\n",
      "[[188  14]\n",
      " [ 40 157]]\n"
     ]
    }
   ],
   "source": [
    "rev = reviews[:400]+reviews[801:]\n",
    "for review,count in rev:\n",
    "    for word in review:\n",
    "        all_train_words.append(word)\n",
    "gnb,feature_set,labels = classification(all_train_words)\n",
    "y_pred = gnb.predict(feature_set[401:800])\n",
    "print(accuracy_score(labels[401:800], y_pred))\n",
    "print(confusion_matrix(labels[401:800], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8225\n",
      "[[177  24]\n",
      " [ 47 152]]\n"
     ]
    }
   ],
   "source": [
    "for review,count in reviews[401:]:\n",
    "    for word in review:\n",
    "        all_train_words.append(word)\n",
    "gnb,feature_set,labels = classification(all_train_words)\n",
    "y_pred = gnb.predict(feature_set[0:400])\n",
    "print(accuracy_score(labels[0:400], y_pred))\n",
    "print(confusion_matrix(labels[0:400], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.8452017543859649\n"
     ]
    }
   ],
   "source": [
    "print('accuracy = ',(0.8395989974937343+0.8421052631578947+0.8571428571428571+0.8646616541353384+0.8225)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 =  0.844\n"
     ]
    }
   ],
   "source": [
    "print('F1 = ',(0.83+0.87+0.84+0.84+0.84)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
