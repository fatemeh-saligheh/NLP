{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from hazm import *\n",
    "from parsivar import Normalizer\n",
    "from parsivar import Tokenizer\n",
    "import os\n",
    "import parsivar\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from parsivar import FindStems\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ZipFile('CA1_Data.zip', 'r') as zip:\n",
    "#     zip.printdir()\n",
    "#     zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_normalizer = parsivar.Normalizer(statistical_space_correction=True)\n",
    "my_tokenizer = Tokenizer()\n",
    "my_stemmer = FindStems()\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def preprocess(paths,entry):\n",
    "    punctuations = '''!()-[]{}،;:'\"\\,<>./?@#$%^&*_~'''\n",
    "    sentences = []\n",
    "#     classi = []\n",
    "    for txt in paths:\n",
    "        file = open('train/'+entry+'/'+txt, encoding=\"utf8\")\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        #remove numbers\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        #normalize\n",
    "        my_normalizer.normalize(text)\n",
    "        #sentence segmentation\n",
    "        sents = my_tokenizer.tokenize_sentences(my_normalizer.normalize(text))\n",
    "        #remove punctuations\n",
    "        for sent in sents:\n",
    "            no_punct = \"\"\n",
    "            for char in sent:\n",
    "                if char not in punctuations:\n",
    "                    no_punct = no_punct + char\n",
    "            sentences.append(no_punct)\n",
    "#             classi.append(entry)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "listDir = os.listdir('train/culture')\n",
    "cultureSentence = preprocess(listDir[:int(80*len(listDir)/100)],'culture')\n",
    "listDir = os.listdir('train/finance')\n",
    "financeSentence = preprocess(listDir[:int(80*len(listDir)/100)],'finance')\n",
    "listDir = os.listdir('train/politic')\n",
    "politicSentence = preprocess(listDir[:int(80*len(listDir)/100)],'politic')\n",
    "listDir = os.listdir('train/social')\n",
    "socialSentence = preprocess(listDir[:int(80*len(listDir)/100)],'social')\n",
    "listDir = os.listdir('train/sport')\n",
    "sportSentence = preprocess(listDir[:int(80*len(listDir)/100)],'sport')\n",
    "listDir = os.listdir('train/technology')\n",
    "technologySentence = preprocess(listDir[:int(80*len(listDir)/100)],'technology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultureWord = []\n",
    "for sent in cultureSentence:\n",
    "    words = my_tokenizer.tokenize_words(sent)\n",
    "    stemWords = []\n",
    "    for word in words:\n",
    "        x=lemmatizer.lemmatize(word)\n",
    "#       \n",
    "        if x.find('#')>=0:\n",
    "            data = x.split(\"#\")\n",
    "            stemWords.append(data[1])\n",
    "        else:\n",
    "            data = x\n",
    "            stemWords.append(data)\n",
    "        \n",
    "        \n",
    "    cultureWord.append(stemWords)\n",
    "    \n",
    "financeWord = []\n",
    "for sent in financeSentence:\n",
    "    words = my_tokenizer.tokenize_words(sent)\n",
    "    stemWords = []\n",
    "    for word in words:\n",
    "        x=lemmatizer.lemmatize(word)\n",
    "        if x.find('#')>=0:\n",
    "            data = x.split(\"#\")\n",
    "            stemWords.append(data[1])\n",
    "        else:\n",
    "            data = x\n",
    "            stemWords.append(data)\n",
    "    financeWord.append(stemWords)\n",
    "    \n",
    "politicWord = []\n",
    "for sent in politicSentence:\n",
    "    words = my_tokenizer.tokenize_words(sent)\n",
    "    stemWords = []\n",
    "    for word in words:\n",
    "        x=lemmatizer.lemmatize(word)\n",
    "        if x.find('#')>=0:\n",
    "            data = x.split(\"#\")\n",
    "            stemWords.append(data[1])\n",
    "        else:\n",
    "            data = x\n",
    "            stemWords.append(data)\n",
    "    politicWord.append(stemWords)\n",
    "    \n",
    "socialWord = []\n",
    "for sent in socialSentence:\n",
    "    words = my_tokenizer.tokenize_words(sent)\n",
    "    stemWords = []\n",
    "    for word in words:\n",
    "        x=lemmatizer.lemmatize(word)\n",
    "        if x.find('#')>=0:\n",
    "            data = x.split(\"#\")\n",
    "            stemWords.append(data[1])\n",
    "        else:\n",
    "            data = x\n",
    "            stemWords.append(data)\n",
    "    socialWord.append(stemWords)\n",
    "    \n",
    "sportWord = []\n",
    "for sent in sportSentence:\n",
    "    words = my_tokenizer.tokenize_words(sent)\n",
    "    stemWords = []\n",
    "    for word in words:\n",
    "        x=lemmatizer.lemmatize(word)\n",
    "        if x.find('#')>=0:\n",
    "            data = x.split(\"#\")\n",
    "            stemWords.append(data[1])\n",
    "        else:\n",
    "            data = x\n",
    "            stemWords.append(data)\n",
    "    sportWord.append(stemWords)\n",
    "\n",
    "technologyWord = []\n",
    "for sent in technologySentence:\n",
    "    words = my_tokenizer.tokenize_words(sent)\n",
    "    stemWords = []\n",
    "    for word in words:\n",
    "        x=lemmatizer.lemmatize(word)\n",
    "        if x.find('#')>=0:\n",
    "            data = x.split(\"#\")\n",
    "            stemWords.append(data[1])\n",
    "        else:\n",
    "            data = x\n",
    "            stemWords.append(data)\n",
    "    technologyWord.append(stemWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultureChar = []\n",
    "for sent in cultureWord:\n",
    "    characters = []\n",
    "    for word in sent:\n",
    "        for chars in [char for char in word]:\n",
    "            characters.append(chars)\n",
    "    cultureChar.append(characters)\n",
    "    \n",
    "sportChar = []\n",
    "for sent in sportSentence:\n",
    "    characters = []\n",
    "    for word in sent:\n",
    "        for chars in [char for char in word]:\n",
    "            characters.append(chars)\n",
    "    sportChar.append([char for char in sent])\n",
    "    \n",
    "financeChar = []\n",
    "for sent in financeSentence:\n",
    "    characters = []\n",
    "    for word in sent:\n",
    "        for chars in [char for char in word]:\n",
    "            characters.append(chars)\n",
    "    financeChar.append([char for char in sent])\n",
    "    \n",
    "politicChar = []\n",
    "for sent in politicSentence:\n",
    "    characters = []\n",
    "    for word in sent:\n",
    "        for chars in [char for char in word]:\n",
    "            characters.append(chars)\n",
    "    politicChar.append([char for char in sent])\n",
    "    \n",
    "socialChar = []\n",
    "for sent in socialSentence:\n",
    "    characters = []\n",
    "    for word in sent:\n",
    "        for chars in [char for char in word]:\n",
    "            characters.append(chars)\n",
    "    socialChar.append([char for char in sent])\n",
    "    \n",
    "technologyChar = []\n",
    "for sent in technologySentence:\n",
    "    characters = []\n",
    "    for word in sent:\n",
    "        for chars in [char for char in word]:\n",
    "            characters.append(chars)\n",
    "    technologyChar.append([char for char in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unigram:\n",
    "    def __init__(self,sentences):\n",
    "        self.corpus_length = 0\n",
    "        self.unigram_frequencies = dict()\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.unigram_frequencies[word] = self.unigram_frequencies.get(word, 0) + 1\n",
    "                self.corpus_length += 1\n",
    "    def unigram_probability(self,word):\n",
    "        return float(self.unigram_frequencies.get(word, 0) + 1)/float(self.corpus_length+len(self.unigram_frequencies)+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram(unigram):\n",
    "    def __init__(self,sentences):\n",
    "        unigram.__init__(self, sentences)\n",
    "        self.corpus_length = 0\n",
    "        self.bigram_frequencies = dict()\n",
    "        self.unique_bigrams = set()\n",
    "        for sentence in sentences:\n",
    "            previous_word = None\n",
    "            for word in sentence:\n",
    "                if previous_word != None:\n",
    "                    self.bigram_frequencies[(previous_word, word)] = self.bigram_frequencies.get((previous_word, word),0) + 1\n",
    "                    self.unique_bigrams.add((previous_word, word))\n",
    "                previous_word = word\n",
    "    def bigram_probability(self,word,previous_word):\n",
    "        return float(self.bigram_frequencies.get((previous_word, word), 0) + 1)/float(self.unigram_frequencies.get(previous_word, 0)+len(self.unigram_frequencies))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 model for word unigram\n",
    "cultureWordUnigram = unigram(cultureWord)\n",
    "financeWordUnigram = unigram(financeWord)\n",
    "politicWordUnigram = unigram(politicWord)\n",
    "socialWordUnigram = unigram(socialWord)\n",
    "sportWordUnigram = unigram(sportWord)\n",
    "technologyWordUnigram = unigram(technologyWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 model for word bigram\n",
    "cultureWordbigram = bigram(cultureWord)\n",
    "financeWordbigram = bigram(financeWord)\n",
    "politicWordbigram = bigram(politicWord)\n",
    "socialWordbigram = bigram(socialWord)\n",
    "sportWordbigram = bigram(sportWord)\n",
    "technologyWordbigram = bigram(technologyWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 model for char unigram\n",
    "cultureCharUnigram = unigram(cultureChar)\n",
    "financeCharUnigram = unigram(financeChar)\n",
    "sportCharUnigram = unigram(sportChar)\n",
    "socialCharUnigram = unigram(socialChar)\n",
    "politicCharUnigram = unigram(politicChar)\n",
    "technologyCharUnigram = unigram(technologyChar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 model for char bigram\n",
    "cultureCharbigram = bigram(cultureChar)\n",
    "financeCharbigram = bigram(financeChar)\n",
    "sportCharbigram = bigram(sportChar)\n",
    "politicCharbigram = bigram(politicChar)\n",
    "socialCharbigram = bigram(socialChar)\n",
    "technologyCharbigram = bigram(technologyChar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_perplexity(model,testset):\n",
    "    perplexity = 0\n",
    "    N = 0\n",
    "    for sent in testset:\n",
    "        for word in sent:\n",
    "            N += 1\n",
    "            perplexity = perplexity + math.log(model.unigram_probability(word),2)\n",
    "    perplexity = perplexity*(-1/float(N))\n",
    "    perplexity = pow(perplexity, 2) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_perplexity(model,testset):\n",
    "    perplexity = 0\n",
    "    N = 0\n",
    "    for sent in testset:\n",
    "        previous_word = None\n",
    "        for word in sent:\n",
    "            if(previous_word != None):\n",
    "                N += 1\n",
    "                perplexity = perplexity + math.log(model.bigram_probability(word,previous_word),2)\n",
    "            previous_word = word\n",
    "    perplexity = perplexity*(-1/float(N))\n",
    "    perplexity = pow(perplexity, 2) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culture\n",
      "finance\n",
      "politic\n",
      "social\n",
      "sport\n",
      "technology\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "mainclass = []\n",
    "entries = os.listdir('train/')\n",
    "for entry in entries[1:]:\n",
    "    print(entry)\n",
    "    listDir = os.listdir('train/'+entry)\n",
    "    for txt in listDir[int(80*len(listDir)/100)+1:]:\n",
    "        files.append(txt)\n",
    "        mainclass.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(files,mainclass,ngram,isword):\n",
    "    predictclass = []\n",
    "    for i in range(len(files)):\n",
    "#         print(i)\n",
    "        SentenceTest = preprocess([files[i]],mainclass[i])\n",
    "        testToken = []\n",
    "        if(isword):\n",
    "            for sent in SentenceTest:\n",
    "                words = my_tokenizer.tokenize_words(sent)\n",
    "                stemWords = []\n",
    "                for word in words:\n",
    "                    x=lemmatizer.lemmatize(word)\n",
    "                    if x.find('#')>=0:\n",
    "                        data = x.split(\"#\")\n",
    "                        stemWords.append(data[1])\n",
    "                    else:\n",
    "                        data = x\n",
    "                        stemWords.append(data)\n",
    "                testToken.append(stemWords)\n",
    "        else:\n",
    "            for sent in SentenceTest:\n",
    "                characters = []\n",
    "                for word in sent:\n",
    "                    for chars in [char for char in word]:\n",
    "                        characters.append(chars)\n",
    "                testToken.append(characters)\n",
    "                \n",
    "        if(ngram == 'unigram'):\n",
    "            culture = unigram_perplexity(cultureWordUnigram,testToken)\n",
    "            politic = unigram_perplexity(politicWordUnigram,testToken)\n",
    "            sport = unigram_perplexity(sportWordUnigram,testToken)\n",
    "            social = unigram_perplexity(socialWordUnigram,testToken)\n",
    "            technology = unigram_perplexity(technologyWordUnigram,testToken)\n",
    "            finance = unigram_perplexity(financeWordUnigram,testToken)\n",
    "        else:\n",
    "            culture = bigram_perplexity(cultureWordbigram,testToken)\n",
    "            politic = bigram_perplexity(politicWordbigram,testToken)\n",
    "            sport = bigram_perplexity(sportWordbigram,testToken)\n",
    "            social = bigram_perplexity(socialWordbigram,testToken)\n",
    "            technology = bigram_perplexity(technologyWordbigram,testToken)\n",
    "            finance = bigram_perplexity(financeWordbigram,testToken)\n",
    "            \n",
    "        if(min(culture,politic,sport,social,technology,finance) == culture ):\n",
    "            predictclass.append('culture')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == sport ):\n",
    "            predictclass.append('sport')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == social ):\n",
    "            predictclass.append('social')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == finance):\n",
    "            predictclass.append('finance')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == politic):\n",
    "            predictclass.append('politic')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == technology):\n",
    "            predictclass.append('technology')\n",
    "    \n",
    "    return predictclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictclass1 = validation(files,mainclass,'unigram',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'file': files,\n",
    "        'mainclass': mainclass,\n",
    "        'predictclass':predictclass1}\n",
    "\n",
    "df = pd.DataFrame(data, columns= ['file', 'mainclass','predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.771186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[44,  5, 13, 16,  0,  6],\n",
       "       [ 0, 74,  6,  0,  0,  4],\n",
       "       [11, 12, 54,  3,  1,  0],\n",
       "       [ 8,  7,  6, 54,  0,  4],\n",
       "       [ 1,  1,  1,  1, 83,  1],\n",
       "       [ 0,  1,  0,  0,  0, 55]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(df['mainclass'],df['predictclass'])\n",
    "print('Accuracy: %f' % accuracy)\n",
    "confusion_matrix(df['mainclass'],df['predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictclass2 = validation(files,mainclass,'bigram',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'file': files,\n",
    "        'mainclass': mainclass,\n",
    "        'predictclass':predictclass2}\n",
    "\n",
    "df = pd.DataFrame(data, columns= ['file', 'mainclass','predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.542373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5, 46,  4,  2,  0, 27],\n",
       "       [ 0, 81,  0,  0,  0,  3],\n",
       "       [ 0, 63, 16,  0,  0,  2],\n",
       "       [ 1, 40,  0, 23,  0, 15],\n",
       "       [ 0,  5,  0,  0, 79,  4],\n",
       "       [ 0,  4,  0,  0,  0, 52]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(df['mainclass'],df['predictclass'])\n",
    "print('Accuracy: %f' % accuracy)\n",
    "confusion_matrix(df['mainclass'],df['predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictclass3 = validation(files,mainclass,'unigram',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'file': files,\n",
    "        'mainclass': mainclass,\n",
    "        'predictclass':predictclass3}\n",
    "\n",
    "df = pd.DataFrame(data, columns= ['file', 'mainclass','predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.182203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[84,  0,  0,  0,  0,  0],\n",
       "       [84,  0,  0,  0,  0,  0],\n",
       "       [81,  0,  0,  0,  0,  0],\n",
       "       [79,  0,  0,  0,  0,  0],\n",
       "       [88,  0,  0,  0,  0,  0],\n",
       "       [54,  0,  0,  0,  0,  2]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(df['mainclass'],df['predictclass'])\n",
    "print('Accuracy: %f' % accuracy)\n",
    "confusion_matrix(df['mainclass'],df['predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictclass4 = validation(files,mainclass,'bigram',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'file': files,\n",
    "        'mainclass': mainclass,\n",
    "        'predictclass':predictclass4}\n",
    "\n",
    "df = pd.DataFrame(data, columns= ['file', 'mainclass','predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.118644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0, 84],\n",
       "       [ 0,  0,  0,  0,  0, 84],\n",
       "       [ 0,  0,  0,  0,  0, 81],\n",
       "       [ 0,  0,  0,  0,  0, 79],\n",
       "       [ 0,  0,  0,  0,  0, 88],\n",
       "       [ 0,  0,  0,  0,  0, 56]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(df['mainclass'],df['predictclass'])\n",
    "print('Accuracy: %f' % accuracy)\n",
    "confusion_matrix(df['mainclass'],df['predictclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def testpreprocess(paths):\n",
    "    punctuations = '''!()-[]{}،;:'\"\\,<>./?@#$%^&*_~'''\n",
    "    sentences = []\n",
    "#     classi = []\n",
    "    for txt in paths:\n",
    "        file = open('test/'+txt, encoding=\"utf8\")\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        #remove numbers\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        #normalize\n",
    "        my_normalizer.normalize(text)\n",
    "        #sentence segmentation\n",
    "        sents = my_tokenizer.tokenize_sentences(my_normalizer.normalize(text))\n",
    "        #remove punctuations\n",
    "        for sent in sents:\n",
    "            no_punct = \"\"\n",
    "            for char in sent:\n",
    "                if char not in punctuations:\n",
    "                    no_punct = no_punct + char\n",
    "            sentences.append(no_punct)\n",
    "#             classi.append(entry)\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(files):\n",
    "    predictclass = []\n",
    "    for i in range(len(files)):\n",
    "#         print(i)\n",
    "        SentenceTest = testpreprocess([files[i]])\n",
    "        testToken = []\n",
    "        for sent in SentenceTest:\n",
    "            words = my_tokenizer.tokenize_words(sent)\n",
    "            stemWords = []\n",
    "            for word in words:\n",
    "                x=lemmatizer.lemmatize(word)\n",
    "        #       \n",
    "                if x.find('#')>=0:\n",
    "                    data = x.split(\"#\")\n",
    "                    stemWords.append(data[1])\n",
    "                else:\n",
    "                    data = x\n",
    "                    stemWords.append(data)\n",
    "            testToken.append(stemWords)\n",
    "\n",
    "        culture = unigram_perplexity(cultureWordUnigram,testToken)\n",
    "        politic = unigram_perplexity(politicWordUnigram,testToken)\n",
    "        sport = unigram_perplexity(sportWordUnigram,testToken)\n",
    "        social = unigram_perplexity(socialWordUnigram,testToken)\n",
    "        technology = unigram_perplexity(technologyWordUnigram,testToken)\n",
    "        finance = unigram_perplexity(financeWordUnigram,testToken)\n",
    "            \n",
    "        if(min(culture,politic,sport,social,technology,finance) == culture ):\n",
    "            predictclass.append('culture')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == sport ):\n",
    "            predictclass.append('sport')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == social ):\n",
    "            predictclass.append('social')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == finance):\n",
    "            predictclass.append('finance')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == politic):\n",
    "            predictclass.append('politic')\n",
    "        elif(min(culture,politic,sport,social,technology,finance) == technology):\n",
    "            predictclass.append('technology')\n",
    "    \n",
    "    return predictclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfiles = os.listdir('test/')\n",
    "predict = test(testfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = {'file': testfiles,\n",
    "            'predictclass':predict}\n",
    "\n",
    "testdf = pd.DataFrame(testdata, columns= ['file','predictclass'])\n",
    "testdf.to_csv('Result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigrm = list(nltk.bigrams(cultureWord[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('علی', 'بهبهانی') 1\n",
      "('بهبهانی', 'در') 1\n",
      "('در', 'گفت\\u200cوگو') 1\n",
      "('گفت\\u200cوگو', 'با') 1\n",
      "('با', 'ایسنا') 1\n",
      "('ایسنا', 'درباره') 1\n",
      "('درباره', 'اخرین') 1\n",
      "('اخرین', 'وضعیت') 1\n",
      "('وضعیت', 'سیمین') 1\n",
      "('سیمین', 'بهبهانی') 1\n",
      "('بهبهانی', 'که') 1\n",
      "('که', 'در') 1\n",
      "('در', 'بخش') 1\n",
      "('بخش', 'مراقبت\\u200cهای') 1\n",
      "('مراقبت\\u200cهای', 'ویژه') 1\n",
      "('ویژه', 'بیمارستان') 1\n",
      "('بیمارستان', 'بستری') 1\n",
      "('بستری', 'است') 1\n",
      "('است', 'گفت') 1\n",
      "('گفت', 'ضریب') 1\n",
      "('ضریب', 'هوشیاری') 1\n",
      "('هوشیاری', 'ایشان') 1\n",
      "('ایشان', 'شش') 1\n",
      "('شش', 'است') 1\n",
      "('است', 'و') 1\n",
      "('و', 'برای') 1\n",
      "('برای', 'این\\u200cکه') 1\n",
      "('این\\u200cکه', 'امیدوار') 1\n",
      "('امیدوار', 'باشیم') 1\n",
      "('باشیم', 'از') 1\n",
      "('از', 'کما\\u200cدربیایند') 1\n",
      "('کما\\u200cدربیایند', 'حداقل') 1\n",
      "('حداقل', 'باید') 1\n",
      "('باید', 'به') 1\n",
      "('به', 'برسد') 1\n"
     ]
    }
   ],
   "source": [
    "#Create your bigrams\n",
    "bgs = nltk.bigrams(cultureWord[1])\n",
    "\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist = nltk.FreqDist(bgs)\n",
    "for k,v in fdist.items():\n",
    "    print(k,v)\n",
    "finder.ngram_fd.viewitems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
